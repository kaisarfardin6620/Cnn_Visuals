{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Data and Preprocessing Visualizations\n",
        "Understanding your data is the first step. These visualizations help you see raw images, augmented versions, and the distribution of your classes.\n",
        "\n",
        "**"
      ],
      "metadata": {
        "id": "I9_3OaTniaM_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtX97AS1iNyI"
      },
      "outputs": [],
      "source": [
        "# --- 1. Data and Preprocessing Visualizations ---\n",
        "\n",
        "print(\"\\n--- Displaying Sample Original Images ---\")\n",
        "# Get a batch of images and labels from the training generator\n",
        "images, labels = next(train_generator)\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    # De-normalize image for display if it was preprocessed to [-1, 1] or [0, 1]\n",
        "    display_image = images[i]\n",
        "    if display_image.min() < 0: # If scaled to [-1, 1]\n",
        "        display_image = (display_image * 0.5 + 0.5) # Scale to [0, 1]\n",
        "    elif display_image.max() > 1: # If not scaled (e.g., from custom_image_preprocessing saving)\n",
        "        display_image = display_image / 255.0 # Scale to [0, 1]\n",
        "\n",
        "    plt.imshow(display_image)\n",
        "    # Get the true label\n",
        "    true_label_idx = np.argmax(labels[i])\n",
        "    plt.title(f\"Class: {class_names[true_label_idx]}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.suptitle(\"Sample Original Images from Training Set\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- Displaying Sample Augmented Images ---\")\n",
        "# To show augmented images, we need to create a temporary generator\n",
        "# that applies the augmentations but doesn't save to disk.\n",
        "temp_aug_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    channel_shift_range=0.1,\n",
        "    fill_mode='nearest',\n",
        "    # Use the combined preprocessing function if it's defined and you want to see its effect\n",
        "    # For visualization, sometimes it's better to see the raw augmentation before model-specific preproc.\n",
        "    # If combined_preprocessing_function expects [0,255] and returns [-1,1], adjust display.\n",
        "    # For simplicity, let's omit model-specific preproc for this visualization.\n",
        "    # preprocessing_function=combined_preprocessing_function\n",
        ")\n",
        "\n",
        "temp_aug_generator = temp_aug_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='image_path',\n",
        "    y_col='label',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=9, # Get 9 images for display\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "images_aug, labels_aug = next(temp_aug_generator)\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    # De-normalize image for display if necessary\n",
        "    display_image_aug = images_aug[i]\n",
        "    if display_image_aug.min() < 0: # If scaled to [-1, 1]\n",
        "        display_image_aug = (display_image_aug * 0.5 + 0.5) # Scale to [0, 1]\n",
        "    plt.imshow(display_image_aug)\n",
        "    true_label_idx_aug = np.argmax(labels_aug[i])\n",
        "    plt.title(f\"Augmented: {class_names[true_label_idx_aug]}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.suptitle(\"Sample Augmented Images from Training Set\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- Class Distribution Plots ---\")\n",
        "# Plotting class distribution for train, validation, and test sets\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "fig.suptitle('Class Distribution Across Splits', fontsize=16)\n",
        "\n",
        "# Training set distribution\n",
        "sns.countplot(y='label', data=train_df, order=train_df['label'].value_counts().index, ax=axes[0], palette='viridis')\n",
        "axes[0].set_title(f'Training Set ({len(train_df)} images)')\n",
        "axes[0].set_xlabel('Number of Images')\n",
        "axes[0].set_ylabel('Class')\n",
        "\n",
        "# Validation set distribution\n",
        "sns.countplot(y='label', data=val_df, order=val_df['label'].value_counts().index, ax=axes[1], palette='viridis')\n",
        "axes[1].set_title(f'Validation Set ({len(val_df)} images)')\n",
        "axes[1].set_xlabel('Number of Images')\n",
        "axes[1].set_ylabel('') # Hide y-label for cleaner look\n",
        "\n",
        "# Test set distribution\n",
        "sns.countplot(y='label', data=test_df, order=test_df['label'].value_counts().index, ax=axes[2], palette='viridis')\n",
        "axes[2].set_title(f'Test Set ({len(test_df)} images)')\n",
        "axes[2].set_xlabel('Number of Images')\n",
        "axes[2].set_ylabel('') # Hide y-label for cleaner look\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "# Image Dimensions Distribution (if you want to analyze actual image sizes before resizing)\n",
        "# This requires loading images and checking their shapes, which can be slow for large datasets.\n",
        "# We'll just print a note here, as the ImageDataGenerator handles resizing.\n",
        "print(\"\\nNote: Image dimensions are handled by target_size in ImageDataGenerator.\")\n",
        "print(f\"All images will be resized to ({IMG_HEIGHT}, {IMG_WIDTH}) for model input.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Architecture Visualizations\n",
        "These help you understand the layers and connections within your CNN.**"
      ],
      "metadata": {
        "id": "PQxzQjxridHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Model Architecture Visualizations ---\n",
        "\n",
        "print(\"\\n--- Model Summary ---\")\n",
        "# Prints a text summary of the model's layers, output shapes, and number of parameters.\n",
        "model.summary()\n",
        "\n",
        "print(\"\\n--- Plotting Model Graph ---\")\n",
        "# Requires graphviz and pydot to be installed:\n",
        "# pip install graphviz pydot\n",
        "# If you encounter errors, ensure graphviz is also installed on your system.\n",
        "try:\n",
        "    plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n",
        "    print(\"Model architecture saved to 'model_architecture.png'.\")\n",
        "    # Display the image in the notebook\n",
        "    from IPython.display import Image\n",
        "    display(Image(filename='model_architecture.png'))\n",
        "except ImportError:\n",
        "    print(\"Warning: pydot and/or graphviz not installed. Cannot plot model architecture.\")\n",
        "    print(\"Please install them: pip install pydot graphviz\")\n",
        "    print(\"Also ensure graphviz is installed on your system and added to PATH.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while plotting the model: {e}\")"
      ],
      "metadata": {
        "id": "rinkEmGeidrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Feature and Interpretability Visualizations\n",
        "These visualizations help you understand what features your CNN is learning and which parts of an image are important for its predictions.**"
      ],
      "metadata": {
        "id": "Eso4fkLcii0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Feature and Interpretability Visualizations ---\n",
        "\n",
        "print(\"\\n--- Visualizing Activation Maps (Feature Maps) ---\")\n",
        "# This shows the output of intermediate layers when an image is passed through the model.\n",
        "# It helps understand what features (edges, textures, patterns) are detected at different depths.\n",
        "\n",
        "# Get a sample image for visualization\n",
        "sample_image, sample_label = next(test_generator)\n",
        "sample_image = sample_image[0] # Take the first image from the batch\n",
        "sample_image_expanded = np.expand_dims(sample_image, axis=0) # Add batch dimension\n",
        "\n",
        "# Select a few convolutional layers to visualize\n",
        "# You might need to adjust layer names based on your model's architecture\n",
        "layer_outputs = [layer.output for layer in model.layers if 'conv2d' in layer.name]\n",
        "activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
        "activations = activation_model.predict(sample_image_expanded)\n",
        "\n",
        "# De-normalize image for display\n",
        "display_image_orig = sample_image\n",
        "if display_image_orig.min() < 0:\n",
        "    display_image_orig = (display_image_orig * 0.5 + 0.5)\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.subplot(len(activations) + 1, 1, 1)\n",
        "plt.imshow(display_image_orig)\n",
        "plt.title(f\"Original Image (True Class: {class_names[np.argmax(sample_label[0])]})\")\n",
        "plt.axis('off')\n",
        "\n",
        "for i, activation in enumerate(activations):\n",
        "    layer_name = model.layers[i].name\n",
        "    # Number of features in the feature map\n",
        "    n_features = activation.shape[-1]\n",
        "    # The feature map has shape (1, size, size, n_features). We want to plot one channel.\n",
        "    size = activation.shape[1]\n",
        "\n",
        "    # Display up to 8 feature maps for each layer (or fewer if n_features is small)\n",
        "    num_to_display = min(n_features, 8)\n",
        "    for j in range(num_to_display):\n",
        "        ax = plt.subplot(len(activations) + 1, num_to_display, num_to_display * (i + 1) + j + 1)\n",
        "        plt.imshow(activation[0, :, :, j], cmap='viridis') # Use 'viridis' or 'gray'\n",
        "        plt.title(f\"{layer_name}\\nChannel {j}\")\n",
        "        plt.axis('off')\n",
        "plt.suptitle(\"Activation Maps for Sample Image\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- Visualizing Filters (Weights) of First Conv Layer ---\")\n",
        "# Visualizing the weights of the first convolutional layer can show what basic patterns\n",
        "# (edges, colors, textures) the network is looking for.\n",
        "\n",
        "# Find the first convolutional layer\n",
        "first_conv_layer = None\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, Conv2D):\n",
        "        first_conv_layer = layer\n",
        "        break\n",
        "\n",
        "if first_conv_layer:\n",
        "    filters, biases = first_conv_layer.get_weights()\n",
        "    print(f\"Shape of filters in '{first_conv_layer.name}': {filters.shape}\") # (kernel_size, kernel_size, input_channels, output_channels)\n",
        "\n",
        "    # Normalize filter values to between 0 and 1 for visualization\n",
        "    f_min, f_max = filters.min(), filters.max()\n",
        "    filters = (filters - f_min) / (f_max - f_min)\n",
        "\n",
        "    # Plot the first few filters\n",
        "    n_filters = filters.shape[-1]\n",
        "    # Display up to 16 filters\n",
        "    num_to_display = min(n_filters, 16)\n",
        "    rows = int(np.ceil(np.sqrt(num_to_display)))\n",
        "    cols = int(np.ceil(num_to_display / rows))\n",
        "\n",
        "    plt.figure(figsize=(cols * 2, rows * 2))\n",
        "    for i in range(num_to_display):\n",
        "        ax = plt.subplot(rows, cols, i + 1)\n",
        "        # Display the filter (assuming 3 input channels, take the first one or average)\n",
        "        if filters.shape[2] == 3: # If input channels are 3 (RGB)\n",
        "            plt.imshow(filters[:, :, :, i])\n",
        "        else: # If input channels are 1 (grayscale) or different, display first channel\n",
        "            plt.imshow(filters[:, :, 0, i], cmap='gray')\n",
        "        plt.title(f'Filter {i+1}')\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(f\"Filters of Layer: {first_conv_layer.name}\", fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No convolutional layer found in the model to visualize filters.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Class Activation Maps (Grad-CAM) ---\")\n",
        "# Grad-CAM helps visualize regions in the input image that are important for the model's prediction.\n",
        "# It requires a target layer (usually the last convolutional layer before classification)\n",
        "# and a target class for which to generate the heatmap.\n",
        "\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    \"\"\"\n",
        "    Generates a Grad-CAM heatmap for a given image and model.\n",
        "    Args:\n",
        "        img_array (np.array): Preprocessed image array (e.g., from test_generator).\n",
        "        model (tf.keras.Model): The trained Keras model.\n",
        "        last_conv_layer_name (str): The name of the last convolutional layer in the model.\n",
        "        pred_index (int, optional): The index of the class for which to generate the heatmap.\n",
        "                                    If None, the predicted class will be used.\n",
        "    Returns:\n",
        "        np.array: The Grad-CAM heatmap.\n",
        "    \"\"\"\n",
        "    # First, we create a model that maps the input image to the activations\n",
        "    # of the last convolutional layer as well as the output predictions.\n",
        "    grad_model = Model(\n",
        "        inputs=model.inputs,\n",
        "        outputs=[model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    # Then, we compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last convolutional layer.\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(preds[0])\n",
        "        class_channel = preds[:, pred_index]\n",
        "\n",
        "    # This is the gradient of the output neuron (top predicted or chosen)\n",
        "    # with respect to the output feature map of the last convolutional layer.\n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel.\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array by \"how important\" this channel is\n",
        "    # with respect to the top predicted class, then sum all the channels to obtain\n",
        "    # the heatmap class activation.\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # For visualization, normalize the heatmap values to between 0 and 1.\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "def display_gradcam(img, heatmap, alpha=0.4):\n",
        "    \"\"\"\n",
        "    Overlays the heatmap on the original image.\n",
        "    Args:\n",
        "        img (np.array): Original image (denormalized, [0, 1] range).\n",
        "        heatmap (np.array): Grad-CAM heatmap ([0, 1] range).\n",
        "        alpha (float): Transparency of the heatmap overlay.\n",
        "    \"\"\"\n",
        "    # Rescale heatmap to a range 0-255\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    # Use viridis colormap to colorize the heatmap\n",
        "    colormap = plt.cm.viridis\n",
        "    heatmap = colormap(heatmap)[:, :, :3] # Get RGB channels\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    # Create a transparent overlay of the heatmap\n",
        "    heatmap_overlay = cv2.addWeighted(img.astype(np.uint8), alpha, heatmap, 1 - alpha, 0)\n",
        "    return heatmap_overlay\n",
        "\n",
        "# Find the last convolutional layer in your model\n",
        "last_conv_layer = None\n",
        "for layer in reversed(model.layers):\n",
        "    if isinstance(layer, Conv2D):\n",
        "        last_conv_layer = layer\n",
        "        break\n",
        "\n",
        "if last_conv_layer:\n",
        "    last_conv_layer_name = last_conv_layer.name\n",
        "    print(f\"Using last convolutional layer: '{last_conv_layer_name}' for Grad-CAM.\")\n",
        "\n",
        "    # Get a sample image and its true label\n",
        "    sample_images_batch, sample_labels_batch = next(test_generator)\n",
        "    sample_image_for_cam = sample_images_batch[0]\n",
        "    true_label_for_cam_idx = np.argmax(sample_labels_batch[0])\n",
        "    true_label_for_cam_name = class_names[true_label_for_cam_idx]\n",
        "\n",
        "    # Predict the class for the sample image\n",
        "    preds = model.predict(np.expand_dims(sample_image_for_cam, axis=0))\n",
        "    predicted_class_idx = np.argmax(preds[0])\n",
        "    predicted_class_name = class_names[predicted_class_idx]\n",
        "    predicted_confidence = preds[0][predicted_class_idx]\n",
        "\n",
        "    # Generate Grad-CAM heatmap\n",
        "    heatmap = make_gradcam_heatmap(\n",
        "        np.expand_dims(sample_image_for_cam, axis=0),\n",
        "        model,\n",
        "        last_conv_layer_name,\n",
        "        pred_index=predicted_class_idx # Generate heatmap for the predicted class\n",
        "    )\n",
        "\n",
        "    # De-normalize image for display\n",
        "    display_img_cam = sample_image_for_cam\n",
        "    if display_img_cam.min() < 0:\n",
        "        display_img_cam = (display_img_cam * 0.5 + 0.5) # Scale to [0, 1] for display\n",
        "    display_img_cam = np.uint8(255 * display_img_cam) # Convert to [0, 255] uint8\n",
        "\n",
        "    # Create and display the Grad-CAM overlay\n",
        "    gradcam_overlay = display_gradcam(display_img_cam, heatmap)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(display_img_cam)\n",
        "    plt.title(f\"Original Image\\nTrue: {true_label_for_cam_name}\\nPred: {predicted_class_name} ({predicted_confidence:.2f})\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(gradcam_overlay)\n",
        "    plt.title(\"Grad-CAM Heatmap Overlay\")\n",
        "    plt.axis('off')\n",
        "    plt.suptitle(\"Grad-CAM Visualization\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Could not find a convolutional layer for Grad-CAM visualization.\")\n",
        "    print(\"Ensure your model has at least one Conv2D layer.\")"
      ],
      "metadata": {
        "id": "No7cnBc7ijKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Performance Visualizations\n",
        "These plots help you assess how well your model learned and how it performs on unseen data.**"
      ],
      "metadata": {
        "id": "gQfuZnWNipHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Model Performance Visualizations ---\n",
        "\n",
        "print(\"\\n--- Training History Plots (Loss & Accuracy) ---\")\n",
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['loss'], label='Training Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.suptitle(\"Model Training History\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "# A confusion matrix shows the number of correct and incorrect predictions\n",
        "# made by the classification model compared to the actual outcomes (true labels).\n",
        "\n",
        "# Get true labels and predictions from the test set\n",
        "print(\"Generating predictions for Confusion Matrix...\")\n",
        "test_labels = test_generator.classes # Get integer labels\n",
        "y_pred_probs = model.predict(test_generator)\n",
        "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Map integer labels back to class names for better readability\n",
        "true_class_names = [class_names[idx] for idx in test_labels]\n",
        "predicted_class_names = [class_names[idx] for idx in y_pred_classes]\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_class_names, predicted_class_names, labels=class_names)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "# Provides precision, recall, F1-score, and support for each class.\n",
        "print(classification_report(true_class_names, predicted_class_names, target_names=class_names))\n",
        "\n",
        "\n",
        "print(\"\\n--- ROC Curve and AUC (One-vs-Rest for Multi-class) ---\")\n",
        "# ROC curve and AUC are useful for evaluating binary classifiers,\n",
        "# but can be extended to multi-class using one-vs-rest strategy.\n",
        "\n",
        "# Get true one-hot encoded labels for ROC curve\n",
        "y_true_one_hot = tf.keras.utils.to_categorical(test_labels, num_classes=len(class_names))\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i, class_name in enumerate(class_names):\n",
        "    fpr, tpr, _ = roc_curve(y_true_one_hot[:, i], y_pred_probs[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'ROC curve of {class_name} (area = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve (One-vs-Rest)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- Sample Predictions with Confidence ---\")\n",
        "# Display a few images with their true label, predicted label, and confidence.\n",
        "\n",
        "# Get a fresh batch from the test generator\n",
        "sample_images_batch_pred, sample_labels_batch_pred = next(test_generator)\n",
        "sample_predictions = model.predict(sample_images_batch_pred)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "for i in range(min(9, len(sample_images_batch_pred))): # Display up to 9 images\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    display_image_pred = sample_images_batch_pred[i]\n",
        "    if display_image_pred.min() < 0:\n",
        "        display_image_pred = (display_image_pred * 0.5 + 0.5)\n",
        "\n",
        "    plt.imshow(display_image_pred)\n",
        "    true_label_idx = np.argmax(sample_labels_batch_pred[i])\n",
        "    predicted_label_idx = np.argmax(sample_predictions[i])\n",
        "    confidence = sample_predictions[i][predicted_label_idx]\n",
        "\n",
        "    true_label_name = class_names[true_label_idx]\n",
        "    predicted_label_name = class_names[predicted_label_idx]\n",
        "\n",
        "    color = \"green\" if true_label_idx == predicted_label_idx else \"red\"\n",
        "    plt.title(f\"True: {true_label_name}\\nPred: {predicted_label_name} ({confidence:.2f})\", color=color)\n",
        "    plt.axis(\"off\")\n",
        "plt.suptitle(\"Sample Predictions with Confidence\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bv8DE4Mlipc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Comparison Visualizations\n",
        "If you train multiple CNN models (e.g., with different architectures, hyperparameters, or datasets), these visualizations help you compare their performance.**"
      ],
      "metadata": {
        "id": "g90Dx1pwizJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Model Comparison Visualizations ---\n",
        "\n",
        "print(\"\\n--- Comparing Training Histories of Multiple Models ---\")\n",
        "# To compare models, you need to have their 'history' objects.\n",
        "# Let's create a dummy history for a second model for demonstration.\n",
        "\n",
        "# Replace with your actual model histories\n",
        "history_model1 = history # Assuming 'history' is from your first model\n",
        "history_model2 = {\n",
        "    'loss': np.random.rand(len(history_model1['loss'])) * 1.8 + 0.6,\n",
        "    'accuracy': np.random.rand(len(history_model1['accuracy'])) * 0.15 + 0.65,\n",
        "    'val_loss': np.random.rand(len(history_model1['val_loss'])) * 1.8 + 0.6,\n",
        "    'val_accuracy': np.random.rand(len(history_model1['val_accuracy'])) * 0.15 + 0.65\n",
        "}\n",
        "# Simulate slightly better performance for model 2\n",
        "history_model2['loss'] = np.sort(history_model2['loss'])[::-1] * 0.9\n",
        "history_model2['val_loss'] = np.sort(history_model2['val_loss'])[::-1] * 0.9\n",
        "history_model2['accuracy'] = np.sort(history_model2['accuracy']) * 1.05\n",
        "history_model2['val_accuracy'] = np.sort(history_model2['val_accuracy']) * 1.05\n",
        "\n",
        "\n",
        "model_histories = {\n",
        "    'Model A': history_model1,\n",
        "    'Model B': history_model2\n",
        "}\n",
        "\n",
        "# Plotting loss comparison\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "for model_name, hist in model_histories.items():\n",
        "    plt.plot(hist['loss'], label=f'{model_name} Training Loss')\n",
        "    plt.plot(hist['val_loss'], linestyle='--', label=f'{model_name} Validation Loss')\n",
        "plt.title('Loss Comparison over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plotting accuracy comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "for model_name, hist in model_histories.items():\n",
        "    plt.plot(hist['accuracy'], label=f'{model_name} Training Accuracy')\n",
        "    plt.plot(hist['val_accuracy'], linestyle='--', label=f'{model_name} Validation Accuracy')\n",
        "plt.title('Accuracy Comparison over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.suptitle(\"Comparison of Model Training Histories\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- Comparing Final Performance Metrics ---\")\n",
        "# Assuming you have final loss and accuracy for each model (e.g., from model.evaluate)\n",
        "# Replace with your actual evaluation results\n",
        "final_metrics = {\n",
        "    'Model A': {'loss': history_model1['val_loss'][-1], 'accuracy': history_model1['val_accuracy'][-1]},\n",
        "    'Model B': {'loss': history_model2['val_loss'][-1], 'accuracy': history_model2['val_accuracy'][-1]},\n",
        "}\n",
        "\n",
        "models = list(final_metrics.keys())\n",
        "losses = [final_metrics[m]['loss'] for m in models]\n",
        "accuracies = [final_metrics[m]['accuracy'] for m in models]\n",
        "\n",
        "x = np.arange(len(models)) # Label locations\n",
        "width = 0.35 # Width of the bars\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "rects1 = ax.bar(x - width/2, losses, width, label='Validation Loss', color='skyblue')\n",
        "rects2 = ax.bar(x + width/2, accuracies, width, label='Validation Accuracy', color='lightcoral')\n",
        "\n",
        "ax.set_ylabel('Value')\n",
        "ax.set_title('Final Validation Performance Comparison')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "ax.set_ylim(0, max(max(losses), max(accuracies)) * 1.1) # Adjust y-lim dynamically\n",
        "\n",
        "def autolabel(rects):\n",
        "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.2f}',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3), # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FKv16alzizaL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}